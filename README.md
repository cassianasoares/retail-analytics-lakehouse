# Retail Analytics Lakehouse

Pipeline end-to-end de anÃ¡lise de dados de varejo utilizando Apache Spark, Delta Lake e Arquitetura Medallion (Bronze, Silver, Gold).

## ğŸ“‹ VisÃ£o Geral

Este projeto implementa um data lakehouse completo para anÃ¡lise de dados de varejo, processando informaÃ§Ãµes de produtos e vendas atravÃ©s de mÃºltiplas camadas de transformaÃ§Ã£o. O objetivo Ã© gerar insights acionÃ¡veis como regras de cross-sell, identificaÃ§Ã£o de produtos com baixa performance e candidatos para promoÃ§Ãµes.

## ğŸ—ï¸ Arquitetura

O projeto segue a **Arquitetura Medallion**, organizada em trÃªs camadas:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Origem    â”‚  CSV Files (Products & Sales)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   Bronze    â”‚  Dados brutos ingeridos com particionamento
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   Silver    â”‚  Dados limpos, transformados e enriquecidos
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚    Gold     â”‚  AgregaÃ§Ãµes analÃ­ticas para consumo
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Camada Bronze
- **IngestÃ£o bruta** dos dados de produtos e vendas
- Dados particionados por `ingestion_date`
- Formato Delta Lake para versionamento e time travel

### Camada Silver
- **Limpeza e transformaÃ§Ã£o** dos dados
- Enriquecimento com colunas derivadas (ano, mÃªs, dia)
- JunÃ§Ã£o de dados de vendas com produtos
- Particionamento por `year`, `month`, `day`

### Camada Gold
- **AgregaÃ§Ãµes analÃ­ticas** prontas para consumo
- TrÃªs principais outputs:
  - **Cross-Sell Rules**: Regras de associaÃ§Ã£o de produtos por dia da semana
  - **Low Sales Products**: Produtos com volume de vendas abaixo do threshold
  - **Promotion Candidates**: Produtos candidatos a campanhas promocionais

## ğŸ› ï¸ Tecnologias

- **Apache Spark 3.5.1**: Processamento distribuÃ­do de dados
- **Delta Lake 3.1.0**: Formato de armazenamento ACID para data lakes
- **Python 3.10.12**: Linguagem de desenvolvimento (gerenciado com pyenv)
- **NumPy 1.26.4**: GeraÃ§Ã£o de dados sintÃ©ticos

## ğŸ“ Estrutura do Projeto

```
retail-analytics-lakehouse/
â”œâ”€â”€ common/
â”‚   â”œâ”€â”€ constants.py          # Constantes e configuraÃ§Ãµes globais
â”‚   â””â”€â”€ spark_session.py      # ConfiguraÃ§Ã£o da sessÃ£o Spark
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ generate_data.py      # Script de geraÃ§Ã£o de dados sintÃ©ticos
â”‚   â””â”€â”€ new_data.py           # Script para geraÃ§Ã£o de novos dados
â”œâ”€â”€ jobs/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â”œâ”€â”€ products_bronze.py    # IngestÃ£o de produtos
â”‚   â”‚   â””â”€â”€ sales_bronze.py       # IngestÃ£o de vendas
â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â”œâ”€â”€ products_silver.py    # TransformaÃ§Ã£o de produtos
â”‚   â”‚   â””â”€â”€ sales_silver.py       # TransformaÃ§Ã£o de vendas
â”‚   â””â”€â”€ gold/
â”‚       â”œâ”€â”€ cross_sell_rules.py        # Regras de cross-sell (FP-Growth)
â”‚       â”œâ”€â”€ lower_sales_products.py    # Produtos com baixa venda
â”‚       â””â”€â”€ promotion_candidates.py    # Candidatos a promoÃ§Ã£o
â”œâ”€â”€ main.py                   # OrquestraÃ§Ã£o do pipeline
â”œâ”€â”€ requirements.txt          # DependÃªncias Python
â””â”€â”€ README.md                # Este arquivo
```

## ğŸš€ Como Executar

### 1. PrÃ©-requisitos

- **Python 3.10.12** (recomendado usar pyenv para gerenciamento de versÃµes)
- **Java 8 ou 11** (requisito do Spark)

### 2. ConfiguraÃ§Ã£o do Ambiente Python

#### Usando pyenv (recomendado)

```bash
# Instale a versÃ£o especÃ­fica do Python
pyenv install 3.10.12

# Configure a versÃ£o local do projeto
pyenv local 3.10.12

# Verifique a versÃ£o
python --version  # Deve exibir: Python 3.10.12
```

### 3. InstalaÃ§Ã£o

```bash
# Clone o repositÃ³rio
git clone <url-do-repositorio>
cd retail-analytics-lakehouse

# Instale as dependÃªncias
pip install -r requirements.txt
```

### 4. Gerar Dados SintÃ©ticos

```bash
python data/generate_data.py
```

Este script cria:
- 120 produtos categorizados (Limpeza, Higiene, Alimentos, Bebidas, etc.)
- 5.000 pedidos com dados de vendas do perÃ­odo janeiro-junho/2025
- Combos de produtos repetidos para facilitar anÃ¡lise de associaÃ§Ã£o
- 10% dos produtos com vendas intencionalmente baixas

### 5. Executar o Pipeline Completo

```bash
python main.py
```

O pipeline executa na seguinte ordem:
1. Bronze: IngestÃ£o de produtos e vendas
2. Silver: Limpeza e transformaÃ§Ã£o
3. Gold: GeraÃ§Ã£o de insights analÃ­ticos

## ğŸ“Š Outputs AnalÃ­ticos

### 1. Cross-Sell Rules
Identifica padrÃµes de compra conjunta usando algoritmo **FP-Growth**:
- Produtos que costumam ser comprados juntos
- Segmentado por dia da semana para campanhas direcionadas
- MÃ©tricas: confidence, lift, support

**Exemplo de uso**: "Clientes que compram cafÃ© Ã s segundas-feiras tÃªm 60% de chance de comprar pÃ£o"

### 2. Low Sales Products
Produtos com volume total de vendas abaixo do threshold (default: 200 unidades):
- Identifica itens com baixa rotaÃ§Ã£o
- Ãštil para gestÃ£o de estoque e descontinuaÃ§Ã£o

### 3. Promotion Candidates
Produtos que combinam:
- Alto volume de vendas (popularidade)
- Baixo faturamento relativo (oportunidade de margem)

Ideal para campanhas promocionais que aumentam receita sem sacrificar volume.

## âš™ï¸ ConfiguraÃ§Ãµes

Principais configuraÃ§Ãµes em `common/constants.py`:

```python
# Threshold para identificar produtos com baixa venda
VALUE_TO_DEFINE_LOW_SALES = 200

# Caminhos do Data Lake
DATA_LAKE = "datalake"
BRONZE_PRODUCTS = f"{DATA_LAKE}/bronze/products"
SILVER_PRODUCTS = f"{DATA_LAKE}/silver/products"
GOLD_CROSS_SELL = f"{DATA_LAKE}/gold/cross_sell_rules"
```

## ğŸ”„ Incrementalidade

O pipeline suporta processamento incremental:
- Novos dados podem ser adicionados com `data/new_data.py`
- Particionamento por data permite processamento eficiente
- Delta Lake oferece operaÃ§Ãµes MERGE e time travel

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a especificada no arquivo [LICENSE](LICENSE).

---

**Desenvolvido para fins educacionais e demonstraÃ§Ã£o de arquitetura de dados moderna.**
